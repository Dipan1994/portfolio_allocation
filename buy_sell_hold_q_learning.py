# -*- coding: utf-8 -*-
"""Buy Sell Hold Q-learning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xibonl-qMnnkBUgEqZOCl6NKOadIJ-66
"""

import numpy as np
import pandas as pd
import tensorflow as tf
import datetime as dt

import os
import logging

import math

from tqdm import tqdm

from sklearn.preprocessing import StandardScaler

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Part 3. AI and ML in Finance/Stocks_Data.csv')
df.set_index('Date',inplace=True)

import random

from collections import deque

import numpy as np
import tensorflow as tf
import keras.backend as K

from keras.models import Sequential
from keras.models import load_model, clone_model
from keras.layers import Dense
from keras.optimizers import Adam

def huber_loss(y_true, y_pred, clip_delta=1.0):
    error = y_true - y_pred
    cond = K.abs(error) <= clip_delta
    squared_loss = 0.5 * K.square(error)
    quadratic_loss = 0.5 * K.square(clip_delta) + clip_delta * (K.abs(error) - clip_delta)
    return K.mean(tf.where(cond, squared_loss, quadratic_loss))

def sigmoid(x):
  try:
    if x < 0:
      return 1 - 1 / (1 + math.exp(x))
    return 1 / (1 + math.exp(-x))
  except Exception as err:
    print("Error in sigmoid: " + err)


def get_state(data, t, n_days):
  """Returns an n-day state representation ending at time t
  """
  d = t - n_days + 1
  block = data[d: t + 1] if d >= 0 else np.append((-d * [data[0]]), data[0: t + 1])  # pad with t0
  # print(block)
  res = []
  for i in range(n_days - 1):
    res.append(sigmoid(block[i + 1] - block[i]))
  return np.array([res])

# 9 * [data[0]] 
# get_state(data,0,10)
# np.append(data[0:1],data[0:1])

class Agent:
  def __init__(self, state_size, strategy="t-dqn", reset_every=1000, pretrained=False, model_name=None):
    self.strategy = strategy

    # agent config
    self.state_size = state_size    	# normalized previous days
    self.action_size = 3           		# [sit, buy, sell]
    self.model_name = model_name
    self.inventory = []
    self.memory = deque(maxlen=10000)
    self.first_iter = True

        # model config
    self.model_name = model_name
    self.gamma = 0.95 # affinity for long term reward
    self.epsilon = 1.0
    self.epsilon_min = 0.01
    self.epsilon_decay = 0.995
    self.learning_rate = 0.001
    self.loss = huber_loss
    self.custom_objects = {"huber_loss": huber_loss}  # important for loading the model from memory
    self.optimizer = Adam(learning_rate=self.learning_rate)

    if pretrained and self.model_name is not None:
      self.model = self.load()
    else:
      self.model = self._model()

    # strategy config
    if self.strategy in ["t-dqn", "double-dqn"]:
      self.n_iter = 1
      self.reset_every = reset_every

    # target network
    self.target_model = clone_model(self.model)
    self.target_model.set_weights(self.model.get_weights())

  def _model(self):
    model = Sequential()
    model.add(Dense(units=128, activation="relu", input_dim=self.state_size))
    model.add(Dense(units=256, activation="relu"))
    model.add(Dense(units=256, activation="relu"))
    model.add(Dense(units=128, activation="relu"))
    model.add(Dense(units=self.action_size))

    model.compile(loss=self.loss, optimizer=self.optimizer)
    return model

  def remember(self, state, action, reward, next_state, done):
    self.memory.append((state, action, reward, next_state, done))

  def act(self, state, is_eval=False):
    # take random action in order to diversify experience at the beginning
    if not is_eval and random.random() <= self.epsilon:
      return random.randrange(self.action_size)

    if self.first_iter:
      self.first_iter = False
      return 1 # make a definite buy on the first iter

    action_probs = self.model.predict(state)
    return np.argmax(action_probs[0])

  def train_experience_replay(self, batch_size):
    mini_batch = random.sample(self.memory, batch_size)
    X_train, y_train = [], []
        
    # DQN
    if self.strategy == "dqn":
      for state, action, reward, next_state, done in mini_batch:
        if done:
          target = reward
        else:
        # approximate deep q-learning equation
          target = reward + self.gamma * np.amax(self.model.predict(next_state)[0])

        # estimate q-values based on current state
        q_values = self.model.predict(state)
        # update the target for current action based on discounted reward
        q_values[0][action] = target

        X_train.append(state[0])
        y_train.append(q_values[0])

    # DQN with fixed targets
    elif self.strategy == "t-dqn":
      if self.n_iter % self.reset_every == 0:
      # reset target model weights
        self.target_model.set_weights(self.model.get_weights())

      for state, action, reward, next_state, done in mini_batch:
        if done:
          target = reward
        else:
        # approximate deep q-learning equation with fixed targets
          target = reward + self.gamma * np.amax(self.target_model.predict(next_state)[0])

        # estimate q-values based on current state
        q_values = self.model.predict(state)
        # update the target for current action based on discounted reward
        q_values[0][action] = target

        X_train.append(state[0])
        y_train.append(q_values[0])

        # Double DQN
    elif self.strategy == "double-dqn":
      if self.n_iter % self.reset_every == 0:
        # reset target model weights
        self.target_model.set_weights(self.model.get_weights())

      for state, action, reward, next_state, done in mini_batch:
        if done:
          target = reward
        else:
          # approximate double deep q-learning equation
          target = reward + self.gamma * self.target_model.predict(next_state)[0][np.argmax(self.model.predict(next_state)[0])]

          # estimate q-values based on current state
        q_values = self.model.predict(state)
        # update the target for current action based on discounted reward
        q_values[0][action] = target

        X_train.append(state[0])
        y_train.append(q_values[0])
                
    else:
      raise NotImplementedError()

        # update q-function parameters based on huber loss gradient
    loss = self.model.fit(
                          np.array(X_train), np.array(y_train),
                          epochs=1, verbose=0
                          ).history["loss"][0]

        # as the training goes on we want the agent to
        # make less random and more optimal decisions
    if self.epsilon > self.epsilon_min:
      self.epsilon *= self.epsilon_decay

    return loss

  def save(self, episode):
    self.model.save("models/{}_{}".format(self.model_name, episode))

  def load(self):
    return load_model("models/" + self.model_name, custom_objects=self.custom_objects)

# Formats Position
format_position = lambda price: ('-$' if price < 0 else '+$') + '{0:.2f}'.format(abs(price))
# Formats Currency
format_currency = lambda price: '${0:.2f}'.format(abs(price))

def train_model(agent, episode, data, ep_count=100, batch_size=32, window_size=10):
  total_profit = 0
  data_length = len(data) - 1

  agent.inventory = []
  avg_loss = []

  state = get_state(data, 0, window_size + 1)

  for t in tqdm(range(data_length), total=data_length, leave=True, desc='Episode {}/{}'.format(episode, ep_count)):        
    reward = 0
    next_state = get_state(data, t + 1, window_size + 1)

    # select an action
    action = agent.act(state)

    # BUY
    if action == 1:
      agent.inventory.append(data[t])

    # SELL
    elif action == 2 and len(agent.inventory) > 0:
      bought_price = agent.inventory.pop(0)
      delta = data[t] - bought_price
      reward = delta #max(delta, 0)
      total_profit += delta

    # HOLD
    else:
      pass

    done = (t == data_length - 1)
    agent.remember(state, action, reward, next_state, done)

    if len(agent.memory) > batch_size:
      loss = agent.train_experience_replay(batch_size)
      avg_loss.append(loss)

    state = next_state

  if episode % 10 == 0:
    agent.save(episode)

  return (episode, ep_count, total_profit, np.mean(np.array(avg_loss)))


def evaluate_model(agent, data, window_size, debug):
  total_profit = 0
  data_length = len(data) - 1

  history = []
  agent.inventory = []
    
  state = get_state(data, 0, window_size + 1)

  for t in range(data_length):        
    reward = 0
    next_state = get_state(data, t + 1, window_size + 1)
        
    # select an action
    action = agent.act(state, is_eval=True)

    # BUY
    if action == 1:
      agent.inventory.append(data[t])

      history.append((data[t], "BUY"))
      if debug:
        logging.debug("Buy at: {}".format(format_currency(data[t])))
        
    # SELL
    elif action == 2 and len(agent.inventory) > 0:
      bought_price = agent.inventory.pop(0)
      delta = data[t] - bought_price
      reward = delta #max(delta, 0)
      total_profit += delta

      history.append((data[t], "SELL"))
      if debug:
        logging.debug("Sell at: {} | Position: {}".format(
                      format_currency(data[t]), format_position(data[t] - bought_price)))
    # HOLD
    else:
      history.append((data[t], "HOLD"))

    done = (t == data_length - 1)
    agent.memory.append((state, action, reward, next_state, done))

    state = next_state
    if done:
      return total_profit, history

window_size = 10
agent = Agent(window_size)
ep_count = 3
data = df['TCS_Close'][-200:].values

for episode in range(ep_count):
  _,_,total_profit,avg_loss = train_model(agent, episode, data, ep_count=ep_count, window_size = window_size)
  print(f'Train Total profit:{total_profit}  \n Average Loss: {avg_loss}')
  total_profit, _ = evaluate_model(agent, data, window_size, False)
  print(f'Evaluate Total profit:{total_profit}')

import altair as alt
import seaborn as sns

total_profit, history = evaluate_model(agent, data, window_size, False)

def visualize(df, history, title="trading session"):
    # add history to dataframe
    position = [history[0][0]] + [x[0] for x in history]
    actions = ['HOLD'] + [x[1] for x in history]
    df['position'] = position
    df['action'] = actions
    
    # specify y-axis scale for stock prices
    scale = alt.Scale(domain=(min(min(df['actual']), min(df['position'])) - 50, max(max(df['actual']), max(df['position'])) + 50), clamp=True)
    
    # plot a line chart for stock positions
    actual = alt.Chart(df).mark_line(
        color='green',
        opacity=0.5
    ).encode(
        x='date:T',
        y=alt.Y('position', axis=alt.Axis(format='$.2f', title='Price'), scale=scale)
    ).interactive(
        bind_y=False
    )
    
    # plot the BUY and SELL actions as points
    points = alt.Chart(df).transform_filter(
        alt.datum.action != 'HOLD'
    ).mark_point(
        filled=True
    ).encode(
        x=alt.X('date:T', axis=alt.Axis(title='Date')),
        y=alt.Y('position', axis=alt.Axis(format='$.2f', title='Price'), scale=scale),
        color='action'
    ).interactive(bind_y=False)

    # merge the two charts
    chart = alt.layer(actual, points, title=title).properties(height=300, width=1000)
    
    return chart

data_prices = df['TCS_Close'][-200:].values
data_dates = df.index[-200:].values
data_viz = pd.DataFrame(zip(data_prices,data_dates), columns =['actual','date'])
position = [history[0][0]] + [x[0] for x in history]
actions = ['HOLD'] + [x[1] for x in history]
data_viz['position'] = position
data_viz['action'] = actions
# len(history)
# visualize(data_viz, history)
scale = alt.Scale(domain=(min(min(data_viz['actual']), min(data_viz['position'])) - 50, 
                          max(max(data_viz['actual']), max(data_viz['position'])) + 50), clamp=True)

actual = alt.Chart(data_viz).mark_line(
                                  color='green',
                                  opacity=0.5
                                ).encode(
                                          x='date',
                                          y=alt.Y('position', axis=alt.Axis(format='$.2f', title='Price'), scale=scale)
                                ).interactive(
                                              bind_y=False
                                )
points = alt.Chart(data_viz).transform_filter(
                                          alt.datum.action != 'HOLD'
                                        ).mark_point(
                                                      filled=True
                                                      ).encode(
                                                                x=alt.X('date', axis=alt.Axis(title='Date')),
                                                                y=alt.Y('position', axis=alt.Axis(format='$.2f', title='Price'), scale=scale),
                                                                color='action'
                                                                ).interactive(bind_y=False)
alt.layer(actual, points, title='trading').properties(height=300, width=1000)

