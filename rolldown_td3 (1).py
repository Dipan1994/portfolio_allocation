# -*- coding: utf-8 -*-
"""Rolldown TD3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cl0Efh-1YjiTZMpKQbH34RYWPRNnM29c
"""

# !pip install pybullet

import numpy as np
import pandas as pd
import tensorflow as tf
import numpy as np
import os
import time
import random
import numpy as np
import matplotlib.pyplot as plt
# import pybullet_envs
import gym

class Environment_Yearly():
  def __init__(self, age, ret_age, plan_age, save_rate, replacement_rate, inv_growth,CL, returns, cov_mat):
    self.start_age = age
    self.age = age
    self.ret_age = ret_age
    self.plan_age = plan_age
    self.save_rate = save_rate
    self.replacement_rate = replacement_rate
    self.wd_rate = 0
    self.inv_growth = inv_growth
    self.returns = returns
    self.cov_mat = cov_mat
    self.income = 1
    self.balance = 0
    self.balance_sim = np.zeros(250)
    self.allocation = [.5,.5]
    self.longevity = 0
    self.done = False
    self.reward = 0
    self.allocations = [self.allocation]
    self.CL = CL
    self.port_returns = []

  def simulate_one_year(self,allocation):
    
    allocation[0] = max(0,allocation[0])
    allocation[1] = max(0,allocation[1])

    if sum(allocation)>0:
      allocation[0]= allocation[0]/sum(allocation)
      allocation[1] = 1- allocation[0]

    returns_sim = np.random.multivariate_normal(self.returns, self.cov_mat, size=250)
    if self.age<self.ret_age:
      self.income = self.income*(1+self.inv_growth)
    elif self.age == self.ret_age:
      self.wd_rate = self.income * self.replacement_rate

    # print(self.balance)
    # balance = self.balance * np.ones([250])
    portfolio_return_sim = []
    for i in range(250):
      if self.age<self.ret_age:
        self.balance_sim[i]  += self.income * self.save_rate
      else:
        # print(self.wd_rate)
        self.balance_sim[i] -= self.wd_rate

      portfolio_return = returns_sim[i][0]*self.allocation[0] + returns_sim[i][1]*self.allocation[1] + returns_sim[i][2]*(1-self.allocation[0]-self.allocation[1])
      portfolio_return_sim.append(portfolio_return)
      self.balance_sim[i] = self.balance_sim[i] *(1+portfolio_return)
    
    self.port_returns = np.percentile(portfolio_return_sim, (100-self.CL))
    # print(np.percentile(portfolio_return_sim, 50))
    # self.balance = np.mean(self.balance_sim)
    self.balance = np.percentile(self.balance_sim,(100-self.CL))

    self.allocations.append(allocation)
    self.longevity = self.age - self.start_age
    self.age +=1
    


  def step(self,action):
    
    self.simulate_one_year(action)

    if self.age == (self.plan_age+1) and self.balance>=0:
      self.done = True
      self.reward = self.longevity + self.balance
    elif self.balance<0:
      self.reward = self.longevity-1
      self.done = True
    else:
      self.reward = self.longevity-1

    return [self.age],self.reward,self.done,0

  def sample(self):
    allocation = np.random.uniform(0,1,2)
    if sum(allocation)>1:
      allocation[0] = allocation[0]/sum(allocation)
      allocation[1] = 1-allocation[0]
    return allocation

  def reset(self):
    self.age = self.start_age
    self.wd_rate = 0
    self.income = 1
    self.balance = 0
    self.allocation = [.5,.5]
    self.longevity = 0
    self.done = False
    self.reward = 0
    self.allocations = [self.allocation]
    self.balance_sim = np.zeros(250)
    return [self.age]

age, ret_age, plan_age, save_rate, replacement_rate, inv_growth,CL, returns, cov_mat = 25,65,95,0.2,0.7,0.015, 90, [0.08,0.04,0.01], [[.18**2,0.04**2,0],[0.04**2,0.08**2,0],[0,0,0.01**2]]
env = Environment_Yearly(age, ret_age, plan_age, save_rate, replacement_rate, inv_growth,CL, returns, cov_mat)
env.sample()
done = False
while not done:
  state,reward,done,_ = env.step([0.4,0.6])
  # print(f"Age:{state[0]-1} \t balance: {env.balance} \t income: {env.income} \t withdrawal: {env.wd_rate}")
print(f'state:{state}')
print(f'reward:{reward}')

class Environment_Full():
  def __init__(self, age, ret_age, plan_age, save_rate, replacement_rate, inv_growth,CL, returns, cov_mat):
    self.start_age = age
    self.age = age
    self.ret_age = ret_age
    self.plan_age = plan_age
    self.wd_rate = 0
    self.save_rate = save_rate
    self.replacement_rate = replacement_rate
    self.inv_growth = inv_growth
    self.returns = returns
    self.cov_mat = cov_mat
    self.income = 1
    self.balance = np.zeros(250)
    self.longevity = np.zeros(250)
    self.done = False
    self.reward = 0
    self.allocations = np.zeros((self.plan_age-self.start_age+1,2))
    self.CL = CL
    self.sim_done = np.zeros(250)

  def simulate(self,allocations):
    self.allocations = np.reshape(allocations,(-1,2))

    for i in self.allocations:
      if i[0]<0:
        i[0] = 0
      if i[1]<0:
        i[1] = 0
      if sum(i)>1:
        i[0] = i[0]/sum(i)
        i[1] = 1 - i[0]

    # print(self.allocations)

    returns_sim = np.random.multivariate_normal(self.returns, self.cov_mat, size=250*self.allocations.shape[0])
    # print(returns_sim.mean(axis=0))
    count = 0

    for i in range(self.start_age,self.plan_age+1):
      self.age = i
      if self.age<self.ret_age:
        self.income = self.income*(1+self.inv_growth)
        self.balance = self.balance + self.income*self.save_rate
      elif self.age==self.ret_age:
        self.wd_rate = self.income * self.replacement_rate
        self.income = 0
        self.balance = self.balance - self.wd_rate
      else:
        self.balance = self.balance - self.wd_rate

      # print(f"{i} /nIncome {np.mean(self.income)}")
      # print(f"{i} /nBalance {np.percentile(self.balance,1-self.CL)}")
      # print(f"{i} /n WD Rate {np.mean(self.wd_rate)}")
      # print(f"{i} /n Replacement Rate {np.mean(self.replacement_rate)}")

      for j in range(250):
        portfolio_return = returns_sim[count][0]*self.allocations[i-self.start_age][0] + returns_sim[count][1]*self.allocations[i-self.start_age][1] + returns_sim[count][2]*(1-self.allocations[i-self.start_age][0]-self.allocations[i-self.start_age][1])
        self.balance[j] = self.balance[j] *(1+portfolio_return)
        if self.balance[j]<=0 and self.sim_done[j]==0:
          self.longevity[j] = i - self.start_age
          self.sim_done[j] = 1
        if self.balance[j]<=0:
          self.balance[j] = 0
          
          # print(f"Sim {j} Balance: {self.balance[j]}   Longevity  {self.longevity[j]}")
        count+=1

    self.longevity = [(self.plan_age - self.start_age+1) if l==0 else l for l in self.longevity]
    # print(self.balance)
    
  def step(self,action):
    
    self.simulate(action)

    self.reward = np.percentile(self.balance,100-self.CL) + np.percentile(self.longevity,100-self.CL) 
    self.done = True    

    return self.plan_age,self.reward,self.done,np.percentile(self.balance,100-self.CL)

  def sample(self):
    rand_allocations = np.random.uniform(0,1,(self.plan_age-self.start_age+1)*2).reshape((-1,2))
    for i in rand_allocations:
      if sum(i)>1:
        i[0] = i[0]/sum(i)
        i[1] = 1-i[0]
    # print(rand_allocations)
    sample_allocations = []
    for i in range(rand_allocations.shape[0]):
      sample_allocations.append(rand_allocations[i][0])
      sample_allocations.append(rand_allocations[i][1])

    return sample_allocations

  def reset(self):
    self.income = 1
    self.age = self.start_age
    self.balance = np.zeros(250)
    self.longevity = np.zeros(250)
    self.done = False
    self.reward = 0
    self.allocations = np.zeros((self.plan_age-self.start_age,2))
    return [self.age]

age, ret_age, plan_age, save_rate, replacement_rate, inv_growth,CL, returns, cov_mat = 25,65,95,0.2,0.7,0.015, 90, [0.08,0.04,0.01], [[.18**2,0.04**2,0],[0.04**2,0.08**2,0],[0,0,0.01**2]]
env = Environment_Full(age, ret_age, plan_age, save_rate, replacement_rate, inv_growth,CL, returns, cov_mat )

done = False

# while not done:
#   a = env.sample()
#   state,reward,done,balance = env.step(env.sample())
#   print(len(a))
#   print(f"Longevity:{state} \t Balance: {balance} \t reward: {env.reward}")

for i in range(5):
  done = False
  env.reset()
  while not done:
    a = env.sample()
    state,reward,done,balance = env.step(env.sample())
    # print(len(a))
    print(f"Plan Age:{state} \t Balance: {balance} \t reward: {env.reward}")



class Critic(tf.keras.Model):
  def __init__(self,action_dim, state_dim):
    super(Critic,self).__init__()
    self.actor_input = tf.keras.layers.Dense(state_dim + action_dim, activation='relu')
    self.c1 = tf.keras.layers.Dense(400,activation='relu')
    self.c2 = tf.keras.layers.Dense(300,activation='relu')
    self.v = tf.keras.layers.Dense(1,activation=None)

  def forward(self,input_state, action):
    x = self.actor_input(tf.concat([input_state,action],axis = 1))
    x = self.c1(x)
    x = self.c2(x)
    x = self.v(x)
    return x

class Actor(tf.keras.Model):
  def __init__(self, action_dim, state_dim):
    super(Actor,self).__init__()
    self.critic_input = tf.keras.layers.Dense(state_dim, activation='relu')
    self.c1 = tf.keras.layers.Dense(400,activation='relu')
    self.c2 = tf.keras.layers.Dense(300,activation='relu')
    self.policy = tf.keras.layers.Dense(action_dim,activation='tanh')
  
  def forward(self, input_state):
    # print(input_state)
    x = self.critic_input(input_state)
    x = self.c1(x)
    x = self.c2(x)
    x = self.policy(x)
    return x

class ReplayBuffer(object):
  def __init__(self, max_size=1e6):
    self.storage = []
    self.max_size = max_size
    self.ptr = 0

  def add(self,transition):
    if len(self.storage)==self.max_size:
      self.storage[int(self.ptr)] = transition
      self.ptr = (self.ptr+1)%self.max_size
    else:
      self.storage.append(transition)

  def sample(self,batch_size=100):
    ind = np.random.randint(0,len(self.storage),size = batch_size)
    batch_states, batch_next_states, batch_rewards,batch_actions, batch_dones = [],[],[],[],[]
    for i in ind:
      state,next_state,reward,action,done = self.storage[i]
      batch_states.append(np.array(state,copy = False))
      batch_next_states.append(np.array(next_state,copy = False))
      batch_actions.append(np.array(action,copy = False))
      batch_rewards.append(np.array(reward,copy = False))
      batch_dones.append(np.array(done,copy = False))
    return np.array(batch_states),np.array(batch_next_states),np.array(batch_rewards).reshape(-1,1),np.array(batch_actions),np.array(batch_dones).reshape(-1,1)

class Agent():
  def __init__(self,action_dim, state_dim, max_iterations, action_min, action_max, 
               warm_up = 10000, batch_size=32, learning_rate=0.01, memory_size=100000, tau = 0.05,
               policy_noise = 0.001, discount = 0.99, actor_update_steps=2):
    self.actor = Actor(action_dim, state_dim)
    self.actor_target = Actor(action_dim, state_dim)

    for t, e in zip(self.actor_target.trainable_variables, self.actor.trainable_variables):
      t.assign(e)

    self.critic_1 = Critic(action_dim, state_dim)
    self.critic_2 = Critic(action_dim, state_dim)
    self.critic_1_target = Critic(action_dim, state_dim)
    self.critic_2_target = Critic(action_dim, state_dim)

    for t, e in zip(self.critic_1_target.trainable_variables,
                    self.critic_2_target.trainable_variables,
                    self.critic_1.trainable_variables,
                    self.critic_2.trainable_variables):
      t.assign(e)
    
    self.action_dim = action_dim
    self.state_dim = state_dim

    self.actor_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    self.critic_1_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)
    self.critic_2_optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)

    self.memory = ReplayBuffer(memory_size)

    self.actor_target.compile(optimizer = self.actor_optimizer)
    self.critic_1_target.compile(optimizer = self.critic_1_optimizer)
    self.critic_1_target.compile(optimizer = self.critic_2_optimizer)

    self.max_iterations = max_iterations
    self.warm_up = warm_up
    self.current_iteration = 0

    self.policy_noise = policy_noise

    self.discount = discount

    self.batch_size = batch_size

    self.actor_update_steps = 2

    self.tau = tau

    self.action_min = action_min
    self.action_max = action_max

  
  def act(self, state, evaluate = False):
    # if self.current_iteration > self.warm_up:
    #         evaluate = True
    state = tf.convert_to_tensor(state, dtype=tf.float32)
    action = self.actor.forward(state)
    if not evaluate:
      action += tf.random.normal(shape=[self.action_dim], mean=0.0, stddev= self.policy_noise)
    
    action = self.action_max * (tf.clip_by_value(action, self.action_min, self.action_max))
    return action[0]

  def update_target(self):
    actor_target_new_weights = []
    actor_targets_weights = self.actor_target.weights
    for i, weight in enumerate(self.actor.weights):
        actor_target_new_weights.append(weight * self.tau + actor_targets_weights[i]*(1-self.tau))
    self.actor_target.set_weights(actor_target_new_weights)

    critic_1_target_new_weights = []
    critic_1_target_weights = self.critic_1_target.weights
    for i, weight in enumerate(self.critic_1.weights):
      critic_1_target_new_weights.append(weight * self.tau + critic_1_target_weights[i]*(1-self.tau))
    self.critic_1_target.set_weights(critic_1_target_new_weights)

    critic_2_target_new_weights = []
    critic_2_target_weights = self.critic_2_target.weights
    for i, weight in enumerate(self.critic_2.weights):
      critic_2_target_new_weights.append(weight * self.tau + critic_2_target_weights[i]*(1-self.tau))
    self.critic_2_target.set_weights(critic_2_target_new_weights)

    
  def train(self):
    if len(self.memory.storage) < self.batch_size:
      return

    states, next_states, rewards, actions, dones = self.memory.sample(self.batch_size)
    # print(states.shape)
    # print(actions.shape)

    states = tf.convert_to_tensor(states, dtype= tf.float32)
    next_states = tf.convert_to_tensor(next_states, dtype= tf.float32)
    rewards = tf.convert_to_tensor(rewards, dtype= tf.float32)
    actions = tf.convert_to_tensor(actions, dtype= tf.float32)
    #dones = tf.convert_to_tensor(dones, dtype= tf.bool)

    with tf.GradientTape() as GT1, tf.GradientTape() as GT2:
      target_actions = self.actor_target.forward(next_states)
      target_actions += tf.random.normal(shape=[*np.shape(target_actions)], mean=0.0, stddev= self.policy_noise)
      target_actions = self.action_max * (tf.clip_by_value(target_actions, self.action_min, self.action_max))

      target_v1 = tf.squeeze(self.critic_1_target.forward(next_states,target_actions),1)
      target_v2 = tf.squeeze(self.critic_2_target.forward(next_states,target_actions),1)

      target_v = tf.math.minimum(target_v1,target_v2)
      # print(target_v.shape)
      # print(rewards.shape)
      # # print(self.discount.shape)
      # print(dones.shape)
      target_v = rewards + self.discount * target_v * (1-dones)

      critic_v1 = tf.squeeze(self.critic_1.forward(states,actions))
      critic_v2 = tf.squeeze(self.critic_2.forward(states,actions))

      critic_loss_1 = tf.keras.losses.MSE(target_v,critic_v1)
      critic_loss_2 = tf.keras.losses.MSE(target_v,critic_v2)

    Grad_1 = GT1.gradient(critic_loss_1,self.critic_1.trainable_variables)
    Grad_2 = GT2.gradient(critic_loss_2,self.critic_2.trainable_variables)

    self.critic_1_optimizer.apply_gradients(zip(Grad_1,self.critic_1.trainable_variables))
    self.critic_2_optimizer.apply_gradients(zip(Grad_2,self.critic_2.trainable_variables))

    self.current_iteration+=1
    # print(self.current_iteration)
    if self.current_iteration%self.actor_update_steps == 0:
      
      with tf.GradientTape() as GT:
        # print(states.shape)
        policies = self.actor.forward(states)
        actor_loss = tf.math.reduce_mean(-self.critic_1.forward(states,policies))

      Grad_actor = GT.gradient(actor_loss,self.actor.trainable_variables)
      self.actor_optimizer.apply_gradients(zip(Grad_actor,self.actor.trainable_variables))

      self.update_target()

def evaluate_policy(agent, eval_period=10):
  avg_reward = 0
  for _ in range(eval_period):
    obs=env.reset()
    print(obs)
    done=False
    while not done:
      action = agent.act([obs])
      obs,reward,done,_ = env.step(np.array(action))
    # print(reward)
    avg_reward += reward
  avg_reward = avg_reward/eval_period
  print('Average reward over the Evaluation step %f'% (avg_reward))
  return avg_reward

tf.random.set_seed(336699)

#Yearly
age, ret_age, plan_age, save_rate, replacement_rate, inv_growth,CL, returns, cov_mat = 25,65,95,0.2,0.7,0.015, 90, [0.08,0.04,0.01], [[.18**2,0.04**2,0],[0.04**2,0.08**2,0],[0,0,0.01**2]]
env = Environment_Yearly(age, ret_age, plan_age, save_rate, replacement_rate, inv_growth,CL, returns, cov_mat)

max_iterations = 50000
warm_up = 5000
state_dim = 1
action_dim = 2
action_max = 1
action_min = 0

eval_freq  = 5e3

agent = Agent(action_dim, state_dim, max_iterations,action_min,action_max)
ep_reward = []
total_avgr = []
target = False
evaluations = [evaluate_policy(agent)]
print(evaluations)

s = 0
total_reward = 0 
# episode_timesteps = 0 
state = env.reset()
done = False
while s<=max_iterations:

  if s<warm_up:
    action = env.sample()
  else:
    action = agent.act([state])

  next_state, reward, done, _ = env.step(np.array(action))

  done_bool = 1 if done else 0

  agent.memory.add((state, next_state, reward, np.array(action), done_bool))
  agent.train()

  state = next_state
  total_reward += reward

  # episode_timesteps +=1

  if done:
    # print('episode')
    ep_reward.append(total_reward)
    avg_reward = np.mean(ep_reward[-100:])
    total_avgr.append(avg_reward)
    total_reward = 0
    # episode_timesteps = 0
    state = env.reset()

  # if s%1000==0:
  #   print(f'Rolling 100 period average reward at {s} iterations is {avg_reward}')

  if s>1 and s%eval_freq==0:
    print(s)
    evaluations.append(evaluate_policy(agent))
    
  s+=1

evaluate_policy(agent, eval_period=50)
print(env.longevity)
print(env.balance)
print(env.age)
print(env.reward)
np.array(env.allocations[1:])

#Full
age, ret_age, plan_age, save_rate, replacement_rate, inv_growth,CL, returns, cov_mat = 25,65,95,0.2,0.7,0.015, 90, [0.08,0.04,0.01], [[.18**2,0.04**2,0],[0.04**2,0.08**2,0],[0,0,0.01**2]]
env = Environment_Full(age, ret_age, plan_age, save_rate, replacement_rate, inv_growth,CL, returns, cov_mat)

max_iterations = 50000
warm_up = 5000
state_dim = 1
action_dim = (plan_age - age + 1)*2
action_max = 1
action_min = 0
eval_freq  = 5e3

agent = Agent(action_dim, state_dim, max_iterations,action_min,action_max)
ep_reward = []
total_avgr = []
target = False
evaluations = [evaluate_policy(agent)]
print(evaluations)

#Full
s = 0
total_reward = 0 
# episode_timesteps = 0 
state = env.reset()
done = False
while s<=max_iterations:

  if s<warm_up:
    action = env.sample()
  else:
    action = agent.act([state])

  next_state, reward, done, _ = env.step(np.array(action))

  done_bool = 1 if done else 0

  agent.memory.add((state, next_state, reward, np.array(action), done_bool))
  agent.train()

  state = next_state
  total_reward += reward

  # episode_timesteps +=1

  if done:
    # print('episode')
    ep_reward.append(total_reward)
    avg_reward = np.mean(ep_reward[-100:])
    total_avgr.append(avg_reward)
    total_reward = 0
    # episode_timesteps = 0
    state = env.reset()

  # if s%1000==0:
  #   print(f'Rolling 100 period average reward at {s} iterations is {avg_reward}')

  if s>1 and s%eval_freq==0:
    print(s)
    evaluations.append(evaluate_policy(agent))
    
  s+=1