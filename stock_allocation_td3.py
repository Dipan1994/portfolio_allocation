# -*- coding: utf-8 -*-
"""Stock Allocation TD3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1brdabNHZk5BqjV_peAhs7GLoP3L1IlX_
"""

import numpy as np
import pandas as pd
import tensorflow as tf

from sklearn.preprocessing import StandardScaler

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('/content/drive/MyDrive/Part 3. AI and ML in Finance/Stocks_Data.csv')
df.set_index('Date',inplace=True)

scaler = StandardScaler()
scaler.fit(df.values)

class Stock_Environment():
  def __init__(self, action_dim, obs_dim, time_delta, window_length, df):
    self.init_cash = 1000
    self.time_delta = time_delta
    self.window_length = window_length

    self.action_dim = action_dim
    self.obs_dim = obs_dim

    self.df = df.copy()

    self.reset()

  def reset(self):
    # self.action = np.zeros(self.action_dim)
    # self.obs = np.zeros(self.obs_dim)
    self.done = False

    self.current_value = self.init_cash

    self.select_window()

    self.allocations_list = []

    self.reward = 0
    self.rewards_list = []

    return np.array(self.df_window.iloc[0])

  def select_window(self):
    max_windows = self.df.shape[0]-self.window_length-1
    window_start =  int(np.round(np.random.uniform(0,1)*max_windows))
    window_end  = window_start + self.window_length
    self.df_window = self.df.iloc[window_start:(window_end+1)]
    self.current_index = 0

  #allocate at today's close prices and analyze at 0+time_delta close prices
  #TCS, Reliance, HDFC Bank in that order
  def allocate(self,allocations):
    for i in range(len(allocations)):
      if allocations[i] < 0:
        allocations[i] = 0
    sum_allocation = sum(allocations)
    for i in range(len(allocations)):
      if sum_allocation == 0:
        allocations[i] = 1/len(allocations)
      else:
        allocations[i] = allocations[i]/sum_allocation

    self.allocations_list.append(allocations)

    previous_value = self.current_value
    
    prices_start = self.df_window.iloc[self.current_index][['TCS_Close','Reliance_Close','HDFCBank_Close']]
    self.current_index += self.time_delta
    prices_end = self.df_window.iloc[self.current_index][['TCS_Close','Reliance_Close','HDFCBank_Close']]
    period_return = prices_end/prices_start - 1
    portfolio_compound = 1 + allocations[0]*period_return[0] + allocations[1]*period_return[1] + allocations[2]*period_return[2] + allocations[3]*0
    self.current_value = self.current_value * portfolio_compound
    self.reward = self.current_value - previous_value
    self.rewards_list.append(self.reward)

    if self.current_index == self.window_length:
      self.done = True

    if (self.current_index + self.time_delta)>self.window_length:
      self.done = True

  def action(self, allocations):
    self.allocate(allocations)
    return np.array(self.df_window.iloc[self.current_index]),self.reward,self.done
    
  def sample(self):
    sample_allocations = np.random.uniform(0,1,self.action_dim)
    if sum(sample_allocations)>1:
      sum_allocations = sum(sample_allocations)
      for i in range(len(sample_allocations)):
        sample_allocations[i] = sample_allocations[i]/sum_allocations
    return np.array(sample_allocations)

action_dim, obs_dim, time_delta, window_length = 4,90,5,200
env = Stock_Environment(action_dim, obs_dim, time_delta, window_length,df)
env.reset()
while not env.done:
  action = env.sample()
  # print(action)
  next_state,reward,done = env.action(action)
  # print(f'reward : {reward} /t Current Index: {env.current_index} /t Current Value: {env.current_value}')
print(f'Total Reward: {sum(env.rewards_list)}')
print(env.allocations_list)
print(next_state)

class Actor(tf.keras.Model):
    def __init__(self, action_dim):
        super(Actor, self).__init__()
        self.layer1 = tf.keras.layers.Dense(400, activation="relu")
        self.layer2 = tf.keras.layers.Dense(300, activation="relu")
        self.out_layer = tf.keras.layers.Dense(action_dim, activation="tanh")

    def forward(self, state):
        x = self.layer1(state)
        x = self.layer2(x)
        actions = self.out_layer(x)
        return actions

class Critic(tf.keras.Model):
    def __init__(self):
        super(Critic, self).__init__()
        self.layer1 = tf.keras.layers.Dense(400, activation="relu")
        self.layer2 = tf.keras.layers.Dense(300, activation="relu")
        self.out_layer = tf.keras.layers.Dense(1, activation=None)

    def forward(self, state, action):
        x = tf.concat([state, action], axis=1)
        x = self.layer1(x)
        x = self.layer2(x)
        q_value = self.out_layer(x)
        return q_value

class RBuffer():
  def __init__(self, maxsize, state_dim, action_dim):
    self.cnt = 0
    self.maxsize = maxsize
    self.state_memory = np.zeros((maxsize, state_dim), dtype=np.float32)
    self.action_memory = np.zeros((maxsize, action_dim), dtype=np.float32)
    self.reward_memory = np.zeros((maxsize,), dtype=np.float32)
    self.next_state_memory = np.zeros((maxsize, state_dim), dtype=np.float32)
    self.done_memory = np.zeros((maxsize,), dtype= np.bool_)

  def add(self, state, next_state, action, done, reward):
    index = self.cnt % self.maxsize
    self.state_memory[index] = state
    self.action_memory[index] = action
    self.reward_memory[index] = reward
    self.next_state_memory[index] = next_state
    self.done_memory[index] = 1- int(done)
    self.cnt += 1

  def sample(self, batch_size):
    max_mem = min(self.cnt, self.maxsize)
    batch = np.random.choice(max_mem, batch_size, replace= False)  
    states = self.state_memory[batch]
    next_states = self.next_state_memory[batch]
    rewards = self.reward_memory[batch]
    actions = self.action_memory[batch]
    dones = self.done_memory[batch]
    return states, next_states, rewards, actions, dones

def Scale_Fit(a, transform=False):
  # print(a)
  if transform == True:
    return scaler.transform([a])[0]
  else:
    return a

class Agent():
  def __init__(self, action_dim, state_dim, batch_size = 100, actor_lr = 0.001, critic_lr = 0.001, memory_size = 100000,
               gamma = 0.99, actor_update_steps = 2, warmups = 1000, tau = 0.005, policy_noise = 0.01, noise_clip = 0.5, transform = False):
    self.action_dim = action_dim
    self.state_dim = state_dim

    self.actor = Actor(action_dim)
    self.actor_target = Actor(action_dim)
    self.critic_1 = Critic()
    self.critic_2 = Critic()
    self.critic_target_1 = Critic()
    self.critic_target_2 = Critic()
    self.actor_opt = tf.keras.optimizers.Adam(actor_lr)
    self.critic_opt_1 = tf.keras.optimizers.Adam(critic_lr)
    self.critic_opt_2 = tf.keras.optimizers.Adam(critic_lr)
    self.actor_target.compile(optimizer=self.actor_opt)
    self.critic_target_1.compile(optimizer=self.critic_opt_1)
    self.critic_target_2.compile(optimizer=self.critic_opt_2)

    self.memory = RBuffer(memory_size, state_dim, action_dim)

    self.batch_size = batch_size
    self.trainstep = 0
    #self.replace = 5
    self.gamma = gamma
    self.actor_update_steps = actor_update_steps
    self.warmup = warmups
    self.tau = tau
    self.policy_noise = policy_noise
    self.noise_clip = noise_clip
    self.transform = transform
    # print(f'Transform:{self.transform}')

    self.print_once = 0

  def act(self, state, evaluate=False):
    if self.trainstep > self.warmup:
      evaluate = True
    state = tf.convert_to_tensor([state], dtype=tf.float32)
    actions = self.actor.forward(state)
    # if not evaluate:
    #   actions += tf.clip_by_value(tf.random.normal(shape=[self.action_dim], mean=0.0, stddev=self.policy_noise),
    #                               0,self.noise_clip)
    return actions[0]

  def save_xp(self,state, next_state, action, done, reward):
    state = Scale_Fit(state,self.transform)
    next_state = Scale_Fit(next_state,self.transform)
    # print(action)
    self.memory.add(state, next_state, action, done, reward)

  def update_target(self, tau=None):
    if tau is None:
        tau = self.tau

    weights1 = []
    targets1 = self.actor_target.weights
    for i, weight in enumerate(self.actor.weights):
        weights1.append(weight * tau + targets1[i]*(1-tau))
    self.actor_target.set_weights(weights1)

    weights2 = []
    targets2 = self.critic_target_1.weights
    for i, weight in enumerate(self.critic_1.weights):
        weights2.append(weight * tau + targets2[i]*(1-tau))
    self.critic_target_1.set_weights(weights2)


    weights3 = []
    targets3 = self.critic_target_2.weights
    for i, weight in enumerate(self.critic_2.weights):
        weights3.append(weight * tau + targets3[i]*(1-tau))
    self.critic_target_2.set_weights(weights3)


  def train(self):
    if self.memory.cnt < self.batch_size:
      return 

    states, next_states, rewards, actions, dones = self.memory.sample(self.batch_size)
    
    states = tf.convert_to_tensor(states, dtype= tf.float32)
    next_states = tf.convert_to_tensor(next_states, dtype= tf.float32)
    rewards = tf.convert_to_tensor(rewards, dtype= tf.float32)
    actions = tf.convert_to_tensor(actions, dtype= tf.float32)
      #dones = tf.convert_to_tensor(dones, dtype= tf.bool)

    with tf.GradientTape() as tape1, tf.GradientTape() as tape2:
            
      target_actions = self.actor_target.forward(next_states)
      # target_actions += tf.clip_by_value(tf.random.normal(shape=[action_dim], mean=0.0, stddev=self.policy_noise), 0, self.noise_clip)       
          
      target_next_state_values_1 = tf.squeeze(self.critic_target_1.forward(next_states, target_actions), 1)
      target_next_state_values_2 = tf.squeeze(self.critic_target_2.forward(next_states, target_actions), 1)
          
      critic_value_1 = tf.squeeze(self.critic_1.forward(states, actions), 1)
      critic_value_2 = tf.squeeze(self.critic_2.forward(states, actions), 1)
          
      next_state_target_value = tf.math.minimum(target_next_state_values_1, target_next_state_values_2)
          
      target_values = rewards + self.gamma * next_state_target_value * dones
      critic_loss_1 = tf.keras.losses.MSE(target_values, critic_value_1)
      critic_loss_2 = tf.keras.losses.MSE(target_values, critic_value_2)
   
      print(f'Next Q values 1 Critic target 1: {target_next_state_values_1}')
      print(f'Next Q values 2 Critic target 2: {target_next_state_values_2}')
      print(f'Next Q values 1 Critic 1: {critic_value_1}')
      print(f'Next Q values 2 Critic 2: {critic_value_2}')
      print(f'Next Q values combined:{target_values}')
      print(f'Critic loss 1:{critic_loss_1}')
      print(f'Critic loss 2:{critic_loss_2}')

    grads_1 = tape1.gradient(critic_loss_1, self.critic_1.trainable_variables)
    grads_2 = tape2.gradient(critic_loss_2, self.critic_2.trainable_variables)
      
    self.critic_opt_1.apply_gradients(zip(grads_1, self.critic_1.trainable_variables))
    self.critic_opt_2.apply_gradients(zip(grads_2, self.critic_2.trainable_variables))
      
    self.trainstep +=1
      
    if self.trainstep % self.actor_update_steps == 0:     
      with tf.GradientTape() as tape3:
        new_policy_actions = self.actor.forward(states)
        actor_loss = -self.critic_1.forward(states, new_policy_actions)
        actor_loss = tf.math.reduce_mean(actor_loss)
          
      grads3 = tape3.gradient(actor_loss, self.actor.trainable_variables)
      self.actor_opt.apply_gradients(zip(grads3, self.actor.trainable_variables))

    #if self.trainstep % self.replace == 0:
      self.update_target()

action_dim , obs_dim = 4,90
time_delta, window_length = 5,500
warmups = 1
memory_size = 10000
tau = 0.05
actor_update_steps = 2
transform = False
episodes = 2
# class Agent():
#   def __init__(self, action_dim, state_dim, batch_size = 100, actor_lr = 0.001, critic_lr = 0.001, memory_size = 100000,
#                gamma = 0.99, actor_update_steps = 2, warmups = 1000, tau = 0.005, policy_noise = 0.01, noise_clip = 0.5)
tf.random.set_seed(336699)
agent = Agent(action_dim,obs_dim,warmups=warmups,memory_size=memory_size,tau = tau, actor_update_steps=actor_update_steps,transform=transform)
ep_reward = []
total_avgr = []
target = False

env = Stock_Environment(action_dim, obs_dim, time_delta, window_length,df)

for s in range(episodes):
  if target == True:
    break
  total_reward = 0 
  state = env.reset()
  # print(state)
  done = False

  while not done:
    #env.render()
    if s<warmups:
      actions = env.sample()
    else:
      # print(Scale([state])[0])
      actions = agent.act(Scale_Fit(state,transform))
    next_state, reward, done = env.action(np.array(actions))
    agent.save_xp(state, next_state, actions, done, reward)
    agent.train()
    state = next_state
    total_reward += reward
    if done:
      ep_reward.append(total_reward)
      avg_reward = np.mean(ep_reward[-20:])
      total_avgr.append(avg_reward)
      print("total reward after {} steps is {} and avg reward is {}".format(s, total_reward, avg_reward))

# agent.save('/content/drive/MyDrive/Part 3. AI and ML in Finance/Stock_allocation_actor')
agent.memory.action_memory[:1000]

import matplotlib.pyplot as plt
plt.plot(np.linspace(0,episodes-1,episodes), ep_reward, 'r', np.linspace(0,episodes-1,episodes), total_avgr, '--b')
plt.show()

# actor = tf.keras.models.load_model('/content/drive/MyDrive/Part 3. AI and ML in Finance/Stock_allocation_actor')
# actor = agent.actor
env = Stock_Environment(action_dim, obs_dim, 5, df.shape[0]-1,df)
state = env.reset()
row = 0
allocations_list = []
total_reward = 0
while not env.done:
  state = Scale_Fit(state,transform)
  state = tf.convert_to_tensor([state], dtype=tf.float32)
  allocation = agent.actor.forward(state)[0]
  allocations_list.append(np.array(allocation))
  next_state, reward, done = env.action(np.array(allocation))
  total_reward += reward
  state = next_state
  # print(f'Current index:{env.current_index} out of {df.shape[0]}')

print(f'Total Reward: {sum(env.rewards_list)} and also {total_reward}')
print(f'Portfolio Returns: {np.round(env.current_value/env.init_cash - 1,4)*100}%')
print(env.allocations_list)

import plotly.express as px
Allocations = pd.DataFrame(env.allocations_list.reshape(-1,1),
                           np.tile(['TCS','Reliance','HDFC Bank','Cash'],env.allocations_list.shape[0]),
                           df.index,columns=['Allocation','Stock','Date'])

fig = px.area(Allocations, x='Date', y='Allocation',
            color="Stock",
            hover_data=['Date'])
 
fig.show()

agent.memory.action_memory[:1000]
# agent.actor.forward(tf.convert_to_tensor([env.reset()], dtype=tf.float32))

env.sample()

